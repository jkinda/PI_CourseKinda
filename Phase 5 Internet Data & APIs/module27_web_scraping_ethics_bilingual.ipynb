{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d58a5276",
   "metadata": {},
   "source": [
    "# Module 27: Web Scraping Ethics\n",
    "## Module 27 : Éthique du Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab33044",
   "metadata": {},
   "source": [
    "## 1. Why This Matters / 1. Pourquoi c'est important\n",
    "- **English:** Scraping responsibly prevents server overload, respects site rules, and avoids legal issues.\n",
    "- **Français :** Scraper de manière responsable évite de surcharger les serveurs, respecte les règles du site, et prévient les problèmes juridiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d24a23",
   "metadata": {},
   "source": [
    "## 2. Spaced & Interleaved Review / 2. Révision espacée et mélangée\n",
    "- **Flash-back:** What library do we use to parse HTML? / Quelle bibliothèque utilisons-nous pour parser le HTML ?\n",
    "- **Interleaving:** How could you add a delay in a scraping loop? / Comment ajouter une pause dans une boucle de scraping ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a28aac7",
   "metadata": {},
   "source": [
    "## 3. Quick Quiz / 3. Quiz rapide\n",
    "1. True or False: You must always check `robots.txt` before scraping. / Vrai ou Faux : Vous devez toujours vérifier `robots.txt` avant de scraper.\n",
    "2. What HTTP status code shows too many requests? / Quel code HTTP indique trop de requêtes ?\n",
    "3. Name one tool to respect rate limits in Python. / Nommez un outil pour respecter les limites de débit en Python.\n",
    "4. True or False: Scraping behind a login without permission is legal. / Vrai ou Faux : Scraper un site authentifié sans permission est légal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0d19a5",
   "metadata": {},
   "source": [
    "## 4. Learning Objectives / 4. Objectifs d'apprentissage\n",
    "By the end, you can: / À la fin, vous pourrez :\n",
    "- Read and respect `robots.txt` rules. / Lire et respecter les règles de `robots.txt`.\n",
    "- Implement rate limiting to avoid overload. / Mettre en place des limites de débit pour éviter la surcharge.\n",
    "- Understand basic legal considerations. / Comprendre les considérations légales de base.\n",
    "- Apply ethical scraping best practices. / Appliquer les meilleures pratiques d'éthique de scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c204e4b",
   "metadata": {},
   "source": [
    "## 5. Core Content / 5. Contenu principal\n",
    "- **robots.txt**:\n",
    "  ```python\n",
    "  import requests\n",
    "  from urllib.robotparser import RobotFileParser\n",
    "\n",
    "  rp = RobotFileParser()\n",
    "  rp.set_url('https://example.com/robots.txt')\n",
    "  rp.read()\n",
    "  can_fetch = rp.can_fetch('*', 'https://example.com/page')\n",
    "  ```\n",
    "- **Rate limiting**:\n",
    "  ```python\n",
    "  import time\n",
    "  for url in urls:\n",
    "      fetch(url)\n",
    "      time.sleep(1)  # wait 1 second between requests\n",
    "  ```\n",
    "- **Error handling**: catch HTTP 429 and backoff.\n",
    "- **Legal considerations**:\n",
    "  - Terms of Service (ToS) of sites. / Conditions d'utilisation du site.\n",
    "  - Copyright and privacy laws. / Lois sur le droit d'auteur et la vie privée.\n",
    "  - Use APIs when available. / Utiliser les API quand possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f7b57d",
   "metadata": {},
   "source": [
    "## 6. Starter Code (Incomplete) / 6. Code de démarrage (incomplet)\n",
    "Complete the TODOs to check `robots.txt` and add rate limiting. / Complétez les TODO pour vérifier `robots.txt` et ajouter des limites de débit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efa296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ethics_starter.py\n",
    "import time\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import requests\n",
    "\n",
    "rp = RobotFileParser()\n",
    "rp.set_url('https://example.com/robots.txt')\n",
    "rp.read()\n",
    "\n",
    "urls = ['https://example.com/page1', 'https://example.com/page2']\n",
    "for url in urls:\n",
    "    # TODO: check if scraping is allowed rp.can_fetch\n",
    "    # TODO: fetch page with requests.get\n",
    "    # TODO: wait 2 seconds before next request\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6bd31c",
   "metadata": {},
   "source": [
    "## 7. Hands-On Project: Ethical Scraper / 7. Projet pratique : Scraper éthique\n",
    "- **Description:** Write a script that:\n",
    "  1. Reads `robots.txt` of a site.\n",
    "  2. Scrapes pages listed in a list only if allowed.\n",
    "  3. Applies a delay of 1 second between requests.\n",
    "  4. Logs skipped URLs and fetched URLs.\n",
    "- **Rubric / Barème:**\n",
    "  - `robots.txt` compliance: 30% / Respect de `robots.txt` : 30%\n",
    "  - Rate limiting implementation: 30% / Mise en place des limites : 30%\n",
    "  - Logging of actions: 20% / Journalisation des actions : 20%\n",
    "  - Code clarity & comments: 20% / Clarté du code et commentaires : 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f17722",
   "metadata": {},
   "source": [
    "## 8. Stretch Tasks / 8. Tâches supplémentaires\n",
    "- Implement exponential backoff on 429 errors. / Mettre en place un backoff exponentiel sur les erreurs 429.\n",
    "- Add header delays and random user-agent rotation. / Ajouter des délais d'en-tête et rotation d'User-Agent.\n",
    "- Create a report of all scraped URLs and status codes. / Créer un rapport de toutes les URL scrapées et leurs codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cb005c",
   "metadata": {},
   "source": [
    "## 9. Reflection / 9. Réflexion\n",
    "- **Summary:** Why is `robots.txt` important? / Pourquoi `robots.txt` est-il important ?\n",
    "- **Muddiest point:** Any legal terms unclear? / Des termes juridiques peu clairs ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dbd5be",
   "metadata": {},
   "source": [
    "## 10. Resources / 10. Ressources\n",
    "- https://www.robotstxt.org/\n",
    "- https://docs.python.org/3/library/urllib.robotparser.html\n",
    "- https://developers.google.com/search/docs/advanced/robots/intro\n",
    "- https://realpython.com/python-web-scraping-practical-introduction/"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
